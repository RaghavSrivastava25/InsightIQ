# -*- coding: utf-8 -*-
"""InsightIQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IKr6VtJAPAPEFqzg3_LwiPFP_BmtQmJF
"""

pip install gradio

pip install langchain

pip install langchain-openai

pip install pandasai

pip install chromadb

import os
import gradio as gr
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_openai import OpenAI
from langchain_community.utilities import GoogleSerperAPIWrapper
import pandas as pd
from pandasai.llm import OpenAI as PandasOpenAI
from pandasai import SmartDataframe

os.environ['OPENAI_API_KEY'] = 'XXXXX'
os.environ['SERPER_API_KEY'] = 'XXXXX'

# Initialize components
llm = OpenAI(temperature=0.1, verbose=True)
pandas_llm = PandasOpenAI()
search = GoogleSerperAPIWrapper()
embeddings = OpenAIEmbeddings()
vector_db = Chroma(collection_name='document_embeddings')

# Function to chunk large files
def chunk_file(file_path, chunk_size):
    with open(file_path, 'r', encoding='utf-8') as file:
        large_text = file.read()

    num_chunks = len(large_text) // chunk_size + 1
    for i in range(num_chunks):
        start = i * chunk_size
        end = (i + 1) * chunk_size if i < num_chunks - 1 else len(large_text)
        chunk = large_text[start:end]
        part_path = f'split_files/part_{i + 1}.txt'
        with open(part_path, 'w', encoding='utf-8') as part_file:
            part_file.write(chunk)

# Function to process text documents
def process_text(file_path):
    loader = TextLoader(file_path)
    documents = loader.load_and_split()
    embeddings_list = []
    for doc in documents:
        embeddings_list.append(embeddings.embed(doc))
    vector_db.store_embeddings(embeddings_list)

# Function to process PDF documents
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    documents = loader.load_and_split()
    embeddings_list = []
    for doc in documents:
        embeddings_list.append(embeddings.embed(doc))
    vector_db.store_embeddings(embeddings_list)

# Function to handle CSV files
def handle_csv(file_path):
    data = pd.read_csv(file_path)
    df = SmartDataframe(data, config={'llm': pandas_llm})
    return df

# Gradio interface
def document_based_qa(file, question):
    file_size_limit = 10 * 1024 * 1024  # 10 MB
    file_path = f'uploaded_files/{file.name}'
    with open(file_path, 'wb') as f:
        f.write(file.read())

    file_size = os.path.getsize(file_path)
    if file_size > file_size_limit:
        chunk_file(file_path, chunk_size=file_size_limit)

    file_ext = file.name.split('.')[-1]
    if file_ext == 'csv':
        return handle_csv(file_path)
    elif file_ext in ['txt', 'pdf']:
        if file_size > file_size_limit:
            for part_file in os.listdir('split_files'):
                part_file_path = f'split_files/{part_file}'
                if file_ext == 'txt':
                    process_text(part_file_path)
                elif file_ext == 'pdf':
                    process_pdf(part_file_path)
        else:
            if file_ext == 'txt':
                process_text(file_path)
            elif file_ext == 'pdf':
                process_pdf(file_path)

    return answer_question(question)


def answer_question(prompt, file_ext):
    if file_ext == 'csv':
        return df.chat(prompt)
    elif file_ext in ['txt', 'pdf']:
        response = vector_db.retrieve_top_k(prompt)
        if response:
            return response
    search_results = search.run(prompt)
    if search_results:
        return llm(search_results[0]['title'])
    else:
        return "Sorry, I couldn't find an answer."


gr.Interface(document_based_qa,
             inputs=["file", "text"],
             outputs="text",
             title="InsightIQ : Your pocket Finance Bro",
             description="Upload the document you want to analyze and ask questions about it."
            ).launch(share=True)



